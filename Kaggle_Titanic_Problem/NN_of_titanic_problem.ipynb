{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing the modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "train_path='train.csv'\n",
    "test_path='test.csv'\n",
    "train_data=pd.read_csv(train_path)\n",
    "test_data=pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "m_1=len(train_data['PassengerId'])\n",
    "n_1=7                            #no(6) extracted parameters from train.csv #(now 7)was 6 Age added\n",
    "train_array=np.zeros((m_1,n_1),dtype=None)  #extracted array for training with 5 parameters and 1 labels\n",
    "\n",
    "max_age=test_data['Age'].max(skipna=True)\n",
    "min_age=test_data['Age'].min(skipna=True)\n",
    "max_fare=test_data['Fare'].max(skipna=True)\n",
    "min_fare=test_data['Fare'].max(skipna=True) \n",
    "#convert the to a matrix of (m_1,n_1)\n",
    "for i in range(m_1):\n",
    "    survived=train_data['Survived'][i]\n",
    "    train_array[i,0]=int(survived)\n",
    "\n",
    "    pclass=train_data['Pclass'][i]\n",
    "    train_array[i,1]=float(pclass)\n",
    "\n",
    "    sex=train_data['Sex'][i]\n",
    "    if sex=='male':\n",
    "        train_array[i,2]=1\n",
    "    elif sex=='female':\n",
    "        train_array[i,2]=0\n",
    "    \n",
    "    sib=train_data['SibSp'][i]\n",
    "    train_array[i,3]=float(sib)\n",
    "\n",
    "    parch=train_data['Parch'][i]\n",
    "    train_array[i,4]=float(parch)\n",
    "\n",
    "    fare=train_data['Fare'][i]\n",
    "    if math.isnan(fare):\n",
    "        fare=random.randint(np.round(min_fare),np.round(max_fare))\n",
    "    train_array[i,5]=float(fare)\n",
    "\n",
    "    age=train_data['Age'][i]\n",
    "    if math.isnan(age):\n",
    "        age=random.random()*(max_age-min_age)\n",
    "    train_array[i,6]=float(age)\n",
    "\n",
    "\n",
    "\n",
    "m_2=len(test_data['PassengerId']) #number of entry\n",
    "n_2=6                          #number(5) of parameters without passengerID # was 5 .Age added\n",
    "test_array=np.zeros((m_2,n_2),dtype=None)   #array for test from test\n",
    "passenger_id=np.zeros((1,m_2),dtype=int)   #array for passenger_id\n",
    "\n",
    "max_age=test_data['Age'].max(skipna=True)\n",
    "min_age=test_data['Age'].min(skipna=True)\n",
    "max_fare=test_data['Fare'].max(skipna=True)\n",
    "min_fare=test_data['Fare'].max(skipna=True)\n",
    "#convert data into (m_2,n_2) array\n",
    "for i in range(m_2):\n",
    "    \n",
    "    pclass=test_data['Pclass'][i]\n",
    "    test_array[i,0]=float(pclass)\n",
    "\n",
    "    sex=test_data['Sex'][i]\n",
    "    if sex=='male':\n",
    "        test_array[i,1]=1\n",
    "    elif sex=='female':\n",
    "        test_array[i,1]=0\n",
    "    \n",
    "    sib=test_data['SibSp'][i]\n",
    "    test_array[i,2]=float(sib)\n",
    "\n",
    "    parch=test_data['Parch'][i]\n",
    "    test_array[i,3]=float(parch)\n",
    "\n",
    "    fare=test_data['Fare'][i]\n",
    "    if math.isnan(fare):\n",
    "        fare=random.randint(np.round(min_fare),np.round(max_fare))\n",
    "    test_array[i,4]=float(fare)\n",
    "\n",
    "    age=test_data['Age'][i]\n",
    "    if math.isnan(age):\n",
    "        age=random.randint(np.round(min_age),np.round(max_age))\n",
    "    test_array[i,5]=float(age)\n",
    "\n",
    "    Id=test_data['PassengerId'][i]\n",
    "    passenger_id[0,i]=int(Id)\n",
    "\n",
    " \n",
    "#Y=ex_array[:,0:1]\n",
    "input_labels=train_array[:,0:1].T # train lables \n",
    "inp_array=train_array[:,1:]      #input array after pre-processing\n",
    "\n",
    "def normalization(ex_array):\n",
    "    m=ex_array.shape[0]\n",
    "\n",
    "    mu=(1/m)*np.sum(ex_array,axis=0,keepdims=True)\n",
    "    sig=(1/m)*np.sum(ex_array**2,axis=0,keepdims=True)\n",
    "    ex_array=(ex_array-mu)/(sig+1e-11)\n",
    "\n",
    "    return ex_array\n",
    "\n",
    "\n",
    "train_array1=normalization(inp_array)\n",
    "test_array1=normalization(test_array)\n",
    "\n",
    "   \n",
    "input_data=train_array1[:].T #input data in (n,m) ,(n=5,m=891)\n",
    "test_data1=test_array[:].T  #test data eligble for test \n",
    "#labels in (1,m)\n",
    "\n",
    "F1=pd.DataFrame(input_data)\n",
    "F1.to_csv('Newdata1.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7001546977033836\n",
      "0.4481611079227099\n",
      "0.44495210104045785\n",
      "0.44378642180258315\n",
      "0.4431985243111644\n",
      "0.4429612011174917\n",
      "0.4428592804000404\n",
      "0.4427536583080067\n",
      "0.4426405128175211\n",
      "0.4426168676141016\n",
      "accuracy==80.58361391694724\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy_test==66.50717703349282\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=input_data.shape[0]\n",
    "layers_dims=(n,10,1)  #2 layer NN with 10 neurons in hidden layer\n",
    "def param_intalization(layers_dims):\n",
    "    n1,n2,n3=layers_dims\n",
    "    W1=np.random.randn(n2,n1)*np.sqrt(2/n1)\n",
    "    b1=np.zeros((n2,1))\n",
    "    W2=np.random.randn(n3,n2)*np.sqrt(2/n2)\n",
    "    b2=np.zeros((n3,1))\n",
    "\n",
    "    assert(W1.shape==(n2,n1))\n",
    "    assert(W2.shape==(n3,n2))\n",
    "    assert(b1.shape==(n2,1))\n",
    "    assert(b2.shape==(n3,1))\n",
    "\n",
    "    param={\n",
    "        'W1':W1,\n",
    "        'b1':b1,\n",
    "        'W2':W2,\n",
    "        'b2':b2\n",
    "    }\n",
    "    return param\n",
    "\n",
    "\n",
    "\n",
    "#linear activation (Z=W.X+b) #z=(2,m)\n",
    "def linear_forward(A_prev,W,b):\n",
    "    \n",
    "    Z=np.dot(W,A_prev)+b\n",
    "\n",
    "    assert(Z.shape==(W.shape[0],A_prev.shape[1]))\n",
    "\n",
    "    cache=(A_prev,W,b)\n",
    "\n",
    "    return Z,cache\n",
    "\n",
    "#Z=linear_forward(input_data,params['W1'],params['b1'])\n",
    "\n",
    "#print(Z.shape)\n",
    "\n",
    "#sigmoid funciton\n",
    "def sigmoid(Z):\n",
    "    A=1/(1+np.exp(-Z))\n",
    "\n",
    "    assert(A.shape==Z.shape)\n",
    "    activation_cache=(Z,A)\n",
    "\n",
    "    return A, activation_cache\n",
    "\n",
    "def relu(Z):\n",
    "\n",
    "    A=np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape==Z.shape)\n",
    "    activation_cache=(Z,A) \n",
    "\n",
    "    return A,activation_cache\n",
    "\n",
    "def linear_activation_forward(A_prev,W,b,Activation):\n",
    "\n",
    "    Z,linear_cache=linear_forward(A_prev,W,b)\n",
    "\n",
    "    if Activation=='Sigmoid':\n",
    "        A,activation_cache=sigmoid(Z)\n",
    "    elif Activation=='relu':\n",
    "        A,activation_cache=relu(Z)\n",
    "    \n",
    "    assert(A.shape==Z.shape)\n",
    "    \n",
    "\n",
    "    cache=(linear_cache,activation_cache)  ##activation_cache(Z,A),linear_cache(A_prev,W,b)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "def forward_prop(X,params):\n",
    "\n",
    "    caches=[]\n",
    "\n",
    "    W1=params['W1']\n",
    "    b1=params['b1']\n",
    "    W2=params['W2']\n",
    "    b2=params['b2']\n",
    "    #l=len(params)//2\n",
    "\n",
    "    A1,cache=linear_activation_forward(X,W1,b1,'relu')\n",
    "    caches.append(cache)\n",
    "    A2,cache=linear_activation_forward(A1,W2,b2,'Sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    AL=A2\n",
    "    #print(AL.shape)\n",
    "    assert(AL.shape==(1,X.shape[1]))\n",
    "\n",
    "    \n",
    "    return AL, caches\n",
    "#AL=forward_prop(input_data,params)\n",
    "\n",
    "\n",
    "def compute_cost(AL,Y,params,lamb=1.25):\n",
    "\n",
    "    m=Y.shape[1]\n",
    "    #l=lne(params)//2\n",
    "    W1=params['W1']\n",
    "    W2=params['W2']\n",
    "    \n",
    "    reg=np.sum(W1**2)+np.sum(W2**2)\n",
    "\n",
    "    cost=-1/m*np.sum(Y*np.log(AL+1e-10)+ (1-Y)*np.log(1-AL-1e-10))+(lamb/(2*m))*reg\n",
    "\n",
    "    '''j=Y*np.log(AL)+(1-Y)*np.log(1-AL)\n",
    "    cost=(-1/m)*np.sum(j)'''\n",
    "\n",
    "    cost = np.squeeze(cost)   #retruns as number not array\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ,linear_cache):\n",
    "    A_prev,W,b=linear_cache\n",
    "    m=A_prev.shape[1]\n",
    "    \n",
    "    dW=(1/m)*np.dot(dZ,A_prev.T)\n",
    "    db=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "\n",
    "    assert(dA_prev.shape==A_prev.shape)\n",
    "    assert(dW.shape==W.shape)\n",
    "    assert(db.shape==b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z,A = cache\n",
    "    \n",
    "    dZ = dA * A * (1-A)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z,_ = cache\n",
    "    dZ = np.array(dA) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z<0]=0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def linear_activation_backword(dA,cache,activation):\n",
    "    linear_cache,activation_cache=cache\n",
    "    \n",
    "    if activation=='relu':\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "\n",
    "    elif activation=='Sigmoid':\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev,dW,db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_back_prop(AL,Y,caches):\n",
    "\n",
    "    grads={}\n",
    "    #m=AL.shape[1]\n",
    "    #Y=Y.reshape(AL.shape)\n",
    "\n",
    "    dAL=- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    #current_cache=caches[1]\n",
    "\n",
    "    grads[\"dA1\"],grads['dW2'],grads['db2']=linear_activation_backword(dAL,caches[1],activation='Sigmoid')\n",
    "    grads['dA0'],grads['dW1'],grads['db1']=linear_activation_backword(grads['dA1'],caches[0],activation='relu')\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "#update parameter\n",
    "\n",
    "def update_params(X,params,grads,learning_rate=0.3,lamb=1.25):\n",
    "\n",
    "    W1=params['W1']\n",
    "    W2=params['W2']\n",
    "\n",
    "    dW1=grads['dW1']\n",
    "    db1=grads['db1']\n",
    "    dW2=grads['dW2']\n",
    "    db2=grads['db2']\n",
    "    m=X.shape[1]\n",
    "\n",
    "    params['W1']=params['W1']-learning_rate*(dW1+(lamb/m)*W1)\n",
    "    params['b1']=params['b1']-learning_rate*db1\n",
    "    params['W2']=params['W2']-learning_rate*(dW2+(lamb/m)*W2)\n",
    "    params['b2']=params['b2']-learning_rate*db2\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def optimize(X,Y,num_iterations,learning_rate=0.1,lamb=1.25):\n",
    "    costs=[]\n",
    "    n=X.shape[0]\n",
    "    layers_dims=(n,10,1)\n",
    "    params=param_intalization(layers_dims)\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        AL,caches=forward_prop(X,params)\n",
    "\n",
    "        cost=compute_cost(AL,Y,params,lamb=1.25)\n",
    "        #print(cost)\n",
    "        grads=model_back_prop(AL,Y,caches)\n",
    "        params=update_params(X,params,grads,learning_rate,lamb=1.25)\n",
    "\n",
    "        if i%1000==0:\n",
    "            costs.append(cost)\n",
    "            print(cost)\n",
    "        \n",
    "    \n",
    "    return params, costs\n",
    "\n",
    "\n",
    "\n",
    "params,cost=optimize(input_data,input_labels,10000,0.3,1.25) \n",
    "\n",
    "def predict(X,params):\n",
    "    #m=X.shape[1]\n",
    "    #Y_prediction=np.zeros((1,m))\n",
    "    AL,_=forward_prop(X,params)\n",
    "    Y_prediction=np.round(AL)\n",
    "\n",
    "\n",
    "    \n",
    "    assert(Y_prediction.shape==(1,X.shape[1]))\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "predictions=predict(input_data,params)\n",
    "print('accuracy=='+str(np.mean(predictions==input_labels)*100))\n",
    "#print(predictions)\n",
    "predictions1=predict(test_data1,params)\n",
    "print((predictions1[0]))\n",
    "#passenger_ID=test_data['PassengerId']\n",
    "\n",
    "test_ans_data=pd.read_csv('my_submissionk1.csv')\n",
    "\n",
    "#test_ans=test_ans_data.to_dict()\n",
    "\n",
    "test_ans=test_ans_data['Survived'].tolist()\n",
    "print('accuracy_test=='+str(np.mean(predictions1==test_ans)*100))\n",
    "\n",
    "\n",
    "\n",
    "predictions1=predictions1.tolist()\n",
    "passenger_id=passenger_id.tolist()\n",
    "data_f={\n",
    "    'PassengerId':passenger_id[0],\n",
    "    'Survived':predictions1[0]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data=data_f)\n",
    "\n",
    "output.to_csv('my_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
