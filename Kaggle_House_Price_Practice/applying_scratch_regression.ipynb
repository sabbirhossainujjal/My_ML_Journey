{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##inputing the extracted data\n",
    "train_path='New_data_frame.csv'\n",
    "test_path='New_test_data_frame.csv'\n",
    "\n",
    "train_data=pd.read_csv(train_path)\n",
    "test_data=pd.read_csv(test_path)\n",
    "Y_labels=pd.read_csv('Y_labels.csv')\n",
    "\n",
    "ID=pd.read_csv('test.csv')\n",
    "Id=ID['Id']\n",
    "\n",
    "n_train=train_data.shape[1]\n",
    "n_test=test_data.shape[1]\n",
    "\n",
    "m_train=train_data.shape[0]\n",
    "m_test=test_data.shape[0]\n",
    "\n",
    "train_array=np.zeros((m_train,n_train),dtype=None)\n",
    "test_array=np.zeros((m_test,n_test),dtype=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 39)\n",
      "(1459, 39)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###Normalizing input and test data\n",
    "\n",
    "for col in train_data.columns:\n",
    "    #print(col)\n",
    "    #median1=train_data[col].median()\n",
    "    mean1=train_data[col].mean()\n",
    "    sig1=(train_data[col]-mean1)**2\n",
    "    sig=sig1.mean()+1e-11\n",
    "    train_data[col]=(train_data[col]-mean1)/sig\n",
    "\n",
    "\n",
    "for col1 in test_data.columns:\n",
    "    mean2=test_data[col1].mean()\n",
    "    sig2=(test_data[col1]-mean2)**2\n",
    "    sig=sig2.mean()+1e-11\n",
    "    test_data[col1]=(test_data[col1]-mean2)/sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(train_data)\n",
    "#print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 1)\n"
     ]
    }
   ],
   "source": [
    "##Normalizing Y \n",
    "Y=np.zeros((m_train,1),dtype=None)\n",
    "print(Y.shape)\n",
    "k=0\n",
    "\n",
    "for col in Y_labels.columns:\n",
    "    if col=='SalePrice':\n",
    "        Y_median=Y_labels[col].median()\n",
    "        #print(Y_median)\n",
    "\n",
    "        Y[:,0]=Y_labels[col]/(Y_median+1e-11)\n",
    "    \n",
    "    k=k+1\n",
    "\n",
    "\n",
    "Y_median=Y_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##normalize Y_labels\n",
    "'''Y_median=Y_labels.median()\n",
    "Y_labels=Y_labels/Y_median'''\n",
    "Y_labels.shape\n",
    "#print(type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rearranging data\n",
    "\n",
    "i=0\n",
    "for col in train_data.columns:\n",
    "    train_array[:,i]=train_data[col]\n",
    "    i=i+1\n",
    "\n",
    "j=0\n",
    "for col in test_data.columns:\n",
    "    test_array[:,j]=test_data[col]\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(test_array)\n",
    "#print(type(test_array[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10634/764648793.py:150: RuntimeWarning: invalid value encountered in log\n",
      "  cost=-1/m*np.sum(Y*np.log(AL+1e-10)+ (1-Y)*np.log(1-AL-1e-10))+(lamb/(2*m))*reg\n",
      "/tmp/ipykernel_10634/764648793.py:245: RuntimeWarning: divide by zero encountered in divide\n",
      "  AL=- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "9.592715304580352\n",
      "10.690100879174508\n",
      "11.261314071738331\n",
      "11.64196623291089\n",
      "11.925782180884445\n",
      "12.151296536443569\n",
      "12.337801321427225\n",
      "12.496452491259328\n",
      "12.634298102058036\n"
     ]
    }
   ],
   "source": [
    "#n=input_data.shape[0]\n",
    "#layers_dims=(n,10,10,1)  #2 layer NN with 10 neurons in hidden layer\n",
    "def param_intalization(layers_dims):\n",
    "    #n1,n2,n3=layers_dims\n",
    "    param={}\n",
    "    L=len(layers_dims)  #layers_dims will include input X dims so, l will be 1 more then layers(X is not include in layers)\n",
    "    '''W1=np.random.randn(n2,n1)*np.sqrt(2/n1)\n",
    "    b1=np.zeros((n2,1))\n",
    "    W2=np.random.randn(n3,n2)*np.sqrt(2/n2)\n",
    "    b2=np.zeros((n3,1))\n",
    "\n",
    "    assert(W1.shape==(n2,n1))\n",
    "    assert(W2.shape==(n3,n2))\n",
    "    assert(b1.shape==(n2,1))\n",
    "    assert(b2.shape==(n3,1))'''\n",
    "    \n",
    "    for l in range(1,L):  #L is 1 ahead so will cause no problem though range dont get the value L\n",
    "        param['W'+str(l)]=np.random.rand(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n",
    "        param['b'+str(l)]=np.zeros((layers_dims[l],1))\n",
    "\n",
    "        assert(param['W'+str(l)].shape==(layers_dims[l],layers_dims[l-1]))\n",
    "        assert(param['b'+str(l)].shape==(layers_dims[l],1))\n",
    "\n",
    "\n",
    "    '''param={\n",
    "        'W1':W1,\n",
    "        'b1':b1,\n",
    "        'W2':W2,\n",
    "        'b2':b2\n",
    "    }'''\n",
    "    return param\n",
    "\n",
    "#params=param_intalization(input_data.shape[0],1)\n",
    "\n",
    "#print(params['W1'],m)\n",
    "\n",
    "\n",
    "#this will be same for all kind of network (NN,1 layer,2 layer) ####from there\n",
    "#linear activation (Z=W.X+b) #z=(2,m)\n",
    "def linear_forward(A_prev,W,b):\n",
    "    \n",
    "    Z=np.dot(W,A_prev)+b\n",
    "\n",
    "    assert(Z.shape==(W.shape[0],A_prev.shape[1]))\n",
    "\n",
    "    cache=(A_prev,W,b)\n",
    "\n",
    "    return Z,cache\n",
    "\n",
    "#Z=linear_forward(input_data,params['W1'],params['b1'])\n",
    "\n",
    "#print(Z.shape)\n",
    "\n",
    "#sigmoid funciton\n",
    "def sigmoid(Z):\n",
    "    A=1/(1+np.exp(-Z))\n",
    "\n",
    "    assert(A.shape==Z.shape)\n",
    "    activation_cache=(Z,A)\n",
    "\n",
    "    return A, activation_cache\n",
    "\n",
    "def relu(Z):\n",
    "\n",
    "    A=np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape==Z.shape)\n",
    "    activation_cache=(Z,A) \n",
    "\n",
    "    return A,activation_cache\n",
    "\n",
    "def linear_activation_forward(A_prev,W,b,Activation):\n",
    "\n",
    "    Z,linear_cache=linear_forward(A_prev,W,b)\n",
    "\n",
    "    if Activation=='Sigmoid':\n",
    "        A,activation_cache=sigmoid(Z)\n",
    "    elif Activation=='relu':\n",
    "        A,activation_cache=relu(Z)\n",
    "    \n",
    "    assert(A.shape==Z.shape)\n",
    "    \n",
    "\n",
    "    cache=(linear_cache,activation_cache)  ##activation_cache(Z,A),linear_cache(A_prev,W,b)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "###upto there\n",
    "\n",
    "\n",
    "#print(sigmoid(Z))\n",
    "\n",
    "#forward propagation\n",
    "\n",
    "def forward_prop(X,params):\n",
    "\n",
    "    caches=[]\n",
    "\n",
    "    '''W1=params['W1']\n",
    "    b1=params['b1']\n",
    "    W2=params['W2']\n",
    "    b2=params['b2']'''\n",
    "\n",
    "\n",
    "    A=X\n",
    "    L=len(params)//2  #finding number of layers\n",
    "\n",
    "    '''A1,cache=linear_activation_forward(X,W1,b1,'relu')\n",
    "    caches.append(cache)\n",
    "    A2,cache=linear_activation_forward(A1,W2,b2,'Sigmoid')\n",
    "    caches.append(cache)'''\n",
    "\n",
    "    #implementing [linear>relu(*L-1 times)]\n",
    "\n",
    "    for l in range(1,L):  #started from 1 cause W started from 1 end end before L cause last layer will have sigmoid of softmax\n",
    "        A_prev=A\n",
    "        A,cache=linear_activation_forward(A_prev,params['W'+str(l)],params['b'+str(l)],Activation='relu')\n",
    "\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL,cache=linear_activation_forward(A,params['W'+str(L)],params['b'+str(L)],Activation='Sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "\n",
    "\n",
    "    #AL=A2\n",
    "    #print(AL.shape)\n",
    "    assert(AL.shape==(1,X.shape[1]))\n",
    "\n",
    "    \n",
    "    return AL, caches\n",
    "#AL=forward_prop(input_data,params)\n",
    "\n",
    "###this will also not change\n",
    "def compute_cost(AL,Y,params,lamb=1.25):\n",
    "\n",
    "    m=Y.shape[1]\n",
    "    #l=lne(params)//2\n",
    "    '''W1=params['W1']\n",
    "    W2=params['W2']'''\n",
    "    \n",
    "    #reg=np.sum(W1**2)+np.sum(W2**2)\n",
    "\n",
    "    L=len(params)//2\n",
    "    reg=0\n",
    "    for l in range(1,L+1):\n",
    "        reg=reg+np.sum(params['W'+str(l)]**2)\n",
    "\n",
    "\n",
    "    cost=-1/m*np.sum(Y*np.log(AL+1e-10)+ (1-Y)*np.log(1-AL-1e-10))+(lamb/(2*m))*reg\n",
    "\n",
    "    '''j=Y*np.log(AL)+(1-Y)*np.log(1-AL)\n",
    "    cost=(-1/m)*np.sum(j)'''\n",
    "\n",
    "    cost = np.squeeze(cost)   #retruns as number not array\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost\n",
    "\n",
    "#cost=compute_cost(AL,input_labels)\n",
    "#print(cost)\n",
    "\n",
    "def linear_backward(dZ,linear_cache):\n",
    "    A_prev,W,b=linear_cache\n",
    "    m=A_prev.shape[1]\n",
    "    \n",
    "    dW=(1/m)*np.dot(dZ,A_prev.T)\n",
    "    db=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "\n",
    "    assert(dA_prev.shape==A_prev.shape)\n",
    "    assert(dW.shape==W.shape)\n",
    "    assert(db.shape==b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA : post-activation gradient, of any shape\n",
    "    cache : 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z,A = cache\n",
    "    \n",
    "    dZ = dA * A * (1-A)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z,_ = cache\n",
    "    dZ = np.array(dA) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z<0]=0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "\n",
    "def linear_activation_backword(dA,cache,activation):\n",
    "    linear_cache,activation_cache=cache\n",
    "    \n",
    "    if activation=='relu':\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "\n",
    "    elif activation=='sigmoid':\n",
    "        dZ=sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev,dW,db\n",
    "\n",
    "###upto there\n",
    "\n",
    "\n",
    "\n",
    "def model_back_prop(AL,Y,caches):\n",
    "\n",
    "    grads={}\n",
    "    L=len(caches) #finding the length of caches. cache is a list including list for every layers linear cache and activation cache\n",
    "    #m=AL.shape[1]\n",
    "    Y=Y.reshape(AL.shape)\n",
    "\n",
    "    AL=- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    #current_cache=caches[1]\n",
    "\n",
    "    '''grads[\"dA1\"],grads['dW2'],grads['db2']=linear_activation_backword(dAL,caches[1],activation='sigmoid')\n",
    "    grads['dA0'],grads['dW1'],grads['db1']=linear_activation_backword(grads['dA1'],caches[0],activation='relu')'''\n",
    "    \n",
    "    #for last layer back_prop(sigmoid)\n",
    "    #dAL=-1*(np.divide(Y,AL+1e-11)-np.divide(1-Y),(1-AL+1e-11))\n",
    "    dAL=(-1)*(Y/(AL) - (1-Y)/(1-AL))\n",
    "\n",
    "    current_cache=caches[L-1]\n",
    "    grads['dA'+str(L-1)],grads['dW'+str(L)],grads['db'+str(L)]=linear_activation_backword(dAL,current_cache,activation='sigmoid')\n",
    "\n",
    "    ##for rest of the layers(relu)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache=caches[l]\n",
    "        grads['dA'+str(l)],grads['dW'+str(l+1)],grads['db'+str(l+1)]=linear_activation_backword(grads['dA'+str(l+1)],current_cache,activation='relu')\n",
    "\n",
    "    \n",
    "\n",
    "    return grads\n",
    "\n",
    "#grad=grad(AL,input_data,input_labels,params)\n",
    "#print(grad)\n",
    "\n",
    "#update parameter\n",
    "\n",
    "def update_params(X,params,grads,learning_rate=0.01,lamb=1.25):\n",
    "\n",
    "    L=len(params)//2\n",
    "\n",
    "    '''W1=params['W1']\n",
    "    W2=params['W2']\n",
    "\n",
    "    dW1=grads['dW1']\n",
    "    db1=grads['db1']\n",
    "    dW2=grads['dW2']\n",
    "    db2=grads['db2']'''\n",
    "    m=X.shape[1]\n",
    "\n",
    "    for l in range(L):\n",
    "        params['W'+str(l+1)]=params['W'+str(l+1)]-learning_rate*(grads['dW'+str(l+1)])#+(lamb/m)*params['W'+str(l+1)])\n",
    "        params['b'+str(l+1)]=params['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "\n",
    "    '''params['W1']=params['W1']-learning_rate*(dW1+(lamb/m)*W1)\n",
    "    params['b1']=params['b1']-learning_rate*db1\n",
    "    params['W2']=params['W2']-learning_rate*(dW2+(lamb/m)*W2)\n",
    "    params['b2']=params['b2']-learning_rate*db2'''\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def optimize(X,Y,num_iterations,learning_rate=0.001,lamb=0):\n",
    "    costs=[]\n",
    "    n=X.shape[0]\n",
    "    layers_dims=(n,10,10,10,1)\n",
    "    params=param_intalization(layers_dims)\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        AL,caches=forward_prop(X,params)\n",
    "\n",
    "        cost=compute_cost(AL,Y,params,lamb=0)\n",
    "        #print(cost)\n",
    "        grads=model_back_prop(AL,Y,caches)\n",
    "        params=update_params(X,params,grads,learning_rate,lamb=0)\n",
    "\n",
    "        if i%1000==0:\n",
    "            costs.append(cost)\n",
    "            print(cost)\n",
    "        \n",
    "    \n",
    "    return params, costs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data=train_array.T\n",
    "input_labels=Y.T\n",
    "\n",
    "params,cost=optimize(input_data,input_labels,10000,0.3,1.5) \n",
    "\n",
    "def predict(X,params):\n",
    "    #m=X.shape[1]\n",
    "    #Y_prediction=np.zeros((1,m))\n",
    "    AL,_=forward_prop(X,params)\n",
    "    #print(AL)\n",
    "    Y_prediction=np.round(AL)\n",
    "\n",
    "\n",
    "    \n",
    "    assert(Y_prediction.shape==(1,X.shape[1]))\n",
    "\n",
    "    return Y_prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
