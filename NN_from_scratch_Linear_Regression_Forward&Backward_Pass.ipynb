{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for implementing forward pass and backword pass of neural network from scratch.\n",
    "- Motivation behind this notebook was the mathmatical explanation of [this course](https://www.coursera.org/learn/neural-networks-deep-learning/) by Andrew Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary input parameters [Layer Information and hyperameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [5, 5, 2] # list of layer of the neural networks \n",
    "# so this model will have 3 layer of neural network and the last layer is a classification layer\n",
    "# layer dims also inclues input array shape like layer_dims[0] = X.shpae[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the weights\n",
    "\n",
    "### Initialization of the weights\n",
    "- The follwoing function will initialize all the parameters(weights and biases) for the network.\n",
    " - xaviar initalizer is used for initialization of the weights and random for biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_initializer(layer_dims):\n",
    "    \"\"\" This function will initalize all the perameter and stores these parameters\n",
    "        in a dictionary named parameters. \n",
    "        xaviar initalizer is used for initialization for weights parameter and zero initialization for bias parameters.\n",
    "        \n",
    "        Args:\n",
    "        layer_dims(list)= dimensions of all the layer of the neural network\n",
    "        output: all the weights and bais parameter (w1, b1,..... etc)\n",
    "    \"\"\"\n",
    "    parameters= {} # dictionary for all parameters \n",
    "    # parameters will be stored as w1, b1, w2, b2 .....\n",
    "    L= len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        # w(weights) will be shaped as next layer nodes by previous layer nodes for example w1= (n1, n0)\n",
    "        parameters['W'+str(l)]= np.random.rand(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/(layer_dims[l]+layer_dims[l-1]))\n",
    "        # b(bias variable) will be shaped as next layer nodes by 1.\n",
    "        parameters['b'+str(l)] = np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        # check wether the assignment have the desired shape.\n",
    "        assert(parameters['W'+str(l)].shape==(layer_dims[l],layer_dims[l-1]))\n",
    "        assert(parameters['b'+str(l)].shape==(layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.32542753, 0.37364332, 0.22594846, 0.37229411, 0.25005722],\n",
       "        [0.29733825, 0.40022195, 0.18433493, 0.18114278, 0.23502322],\n",
       "        [0.12182328, 0.19385777, 0.28549221, 0.09497587, 0.04772753],\n",
       "        [0.01186503, 0.3293282 , 0.19713594, 0.02197311, 0.4114601 ],\n",
       "        [0.01418737, 0.24916951, 0.43096036, 0.00484937, 0.18539176]]),\n",
       " 'b1': array([[0.07328775],\n",
       "        [0.37671992],\n",
       "        [0.45621285],\n",
       "        [0.66116328],\n",
       "        [0.67549705]]),\n",
       " 'W2': array([[0.41384845, 0.16759992, 0.29326084, 0.22022787, 0.52523349],\n",
       "        [0.51896394, 0.24773936, 0.12588104, 0.28093464, 0.47747299]]),\n",
       " 'b2': array([[0.31731246],\n",
       "        [0.78206464]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer_dims=[5,5,2]\n",
    "param= weight_initializer(layer_dims)\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear forward\n",
    "\n",
    "- This function will compute linear computation of the forward propagation.\n",
    "- More specifically, will compute this: Z= W . X + b (where w=weights, x=input vector, b=bias value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_forward\n",
    "# activation function to that linear_forward \n",
    "# Gets the node activation values\n",
    "\n",
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"This function will compute the linear activation for the nodes of the network \n",
    "        using the values of previous nodes activation values\n",
    "        linear forward function:\n",
    "        Z= W . X + b \n",
    "\n",
    "    Args:\n",
    "        A_prev(nd array) : Previous layer activation values \n",
    "        W : weight matrix for the forward path between two nodes (shape=(n1, n0))\n",
    "        b : bias varibale matrix \n",
    "        \n",
    "    output:\n",
    "        Z: next layer node z values. matrix.\n",
    "    \"\"\"\n",
    "    # Z = np.dot(W, A_prev) + b\n",
    "    Z = np.dot(W, A_prev) \n",
    "    # type(Z)\n",
    "    assert (Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    return Z    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implements all the necessary activation function for NN\n",
    "- These functions will add non-linearity in our linear nn system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the related activation function for the neural network\n",
    "# These functions will add non-linearity in our linear nn system.\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"Computes the sigmoid of the linear function, hence introduce non-linearity.\n",
    "        It will squashed the linear values of the forward path between 0 and 1.\n",
    "    \n",
    "    Args:\n",
    "        Z (nd array): output of the linear forward path function.\n",
    "    output: \n",
    "        A(nd array): activations of the nodes of a layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    A= 1/ (1+np.exp(-Z))\n",
    "    \n",
    "    assert (A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"Computes the tanh (hyporabolic tangent) of the linear function, hence introduce non-linearity.\n",
    "    \n",
    "    Args:\n",
    "        Z(nd array) : output of the linear forward path function.\n",
    "    output: \n",
    "        A(nd array): activations of the nodes of a layer.\n",
    "    \"\"\"\n",
    "    A= np.tanh(Z)\n",
    "    assert (A.shape== Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"Evaluates the ReLU(Rectified Linear Unit) function value of the linear function, hence introduce non-linearity.\n",
    "        ReLU(z) = max(0, z)\n",
    "    Args:\n",
    "        Z (nd array): output of the linear forward path function.\n",
    "    output: \n",
    "        A(nd array): activations of the nodes of a layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    A= np.maximum(0, Z)\n",
    "    assert (A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"Evaluates the leaky ReLU function(extension of ReLU) value of the linear function, hence introduce non-linearity.\n",
    "        LeakyReLU(z) = max(0.01 * z, z)\n",
    "    Args:\n",
    "        Z (nd array): output of the linear forward path function.\n",
    "    output: \n",
    "        A(nd array): activations of the nodes of a layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    A= np.maximum(0.01*Z, Z)\n",
    "    assert (A.shape == Z.shape)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass of single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_function='sigmoid'):\n",
    "    \"\"\" Combines the linear function and activation function to get activations of the nodes\n",
    "    Args:\n",
    "        A_prev\n",
    "        W \n",
    "        b \n",
    "        activation_function (str, optional):Which activation function to use. Defaults to 'sigmoid'.\n",
    "    \"\"\"\n",
    "    # computing Z\n",
    "    Z= linear_forward(A_prev, W, b)\n",
    "    \n",
    "    # computing A\n",
    "    \n",
    "    if activation_function == 'sigmoid' :\n",
    "        A= sigmoid(Z)\n",
    "    elif activation_function == 'tanh' :\n",
    "        A= tanh(Z)\n",
    "    elif activation_function == 'relu' :\n",
    "        A= relu(Z)\n",
    "    elif activation_function == 'leakyrelu' :\n",
    "        A= leaky_relu(Z)\n",
    "    \n",
    "    cache= (Z, A, A_prev, W, b) # all the variables of a forward node in one list\n",
    "                                # These will be needed for back propagation\n",
    "    \n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full forward propagation of full Neural network \n",
    "\n",
    "This function will calcaulte a complte forward propagation of full neural network and giving the prediction as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"This function will execute the full forward path of a neural network and will return prediction or probalility.\n",
    "\n",
    "    Args:\n",
    "        X : Input nd array\n",
    "        parameters: all the parameter to compute activations\n",
    "        \n",
    "    Output:\n",
    "        AL= last layer activation values. (predicted Y)\n",
    "        caches= All necessary values of a node for back propagation. (Z, A, A_prev, W, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches= []\n",
    "    \n",
    "    A= X\n",
    "    L= len(parameters)//2 # finding number of layers. As all layer have w, b parameter.\n",
    "    \n",
    "    #implementing [linear>relu((L-1)times)]\n",
    "    for l in range (1, L): # interates untill the last layer\n",
    "        A_prev= A\n",
    "        A, temp_cache= linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation_function= 'leakyrelu')\n",
    "        \n",
    "        caches.append(temp_cache)\n",
    "    \n",
    "    # last layer\n",
    "    AL, temp_cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation_function= 'sigmoid')\n",
    "    caches.append(temp_cache)\n",
    "    \n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost/loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, parameters, lamb= 1.25):\n",
    "    \"\"\" Computs loss/ cost of the forward path\n",
    "\n",
    "    Args:\n",
    "        AL : predicted label\n",
    "        Y : Actual label\n",
    "        parameters \n",
    "        lamb (float, optional): regularization constant lambda. Defaults to 1.25.\n",
    "    \"\"\"\n",
    "    \n",
    "    m= Y.shape[1] # output layer node number/ class of classifier\n",
    "    \n",
    "    L= len(parameters)//2 # no of layers\n",
    "    \n",
    "    # regularization part\n",
    "    #\"https://towardsdatascience.com/regularization-an-important-concept-in-machine-learning-5891628907ea#:~:text=Regularization%20is%20a%20technique%20used,don't%20take%20extreme%20values.\"\n",
    "    \n",
    "    reg= 0 \n",
    "    for l in range(1, L+1) :\n",
    "        reg= reg + np.sum(parameters['W'+ str(l)]**2)\n",
    "    \n",
    "    cost=(-1/m)*np.sum(Y*np.log(AL+1e-10) + (1 - Y) * np.log(1 - AL-1e-10)) + (lamb/(2*m)) * reg\n",
    "    # 1e-10 is added in log sothat zero can go into log\n",
    "    \n",
    "    cost= np.squeeze(cost) # cost must be a number rather than a array.\n",
    "    assert(cost.shape== ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking everything working fine\n",
    "\n",
    "K= np.array([[1], [1.01], [0], [2], [5]]) #input\n",
    "parameters= weight_initializer(layer_dims) # initialization\n",
    "AL, caches= forward_propagation(K, parameters) # compute prediction\n",
    "Y= np.array([[0],[1]])\n",
    "c= compute_cost(AL, Y, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.30690338, 0.11173668, 0.44643018, 0.08852416, 0.02707642],\n",
       "        [0.16384467, 0.25581957, 0.02770852, 0.18784855, 0.29387433],\n",
       "        [0.25077069, 0.35368401, 0.4163648 , 0.02162521, 0.44599713],\n",
       "        [0.18581151, 0.1432331 , 0.00589277, 0.06285917, 0.26218866],\n",
       "        [0.10360754, 0.24546001, 0.06052531, 0.00656289, 0.42867652]]),\n",
       " 'b1': array([[0.38185299],\n",
       "        [0.2237979 ],\n",
       "        [0.21490264],\n",
       "        [0.26438148],\n",
       "        [0.78754164]]),\n",
       " 'W2': array([[0.50356514, 0.32030772, 0.52615517, 0.47563611, 0.10438827],\n",
       "        [0.31259295, 0.00123094, 0.067075  , 0.03374472, 0.39424899]]),\n",
       " 'b2': array([[0.38944142],\n",
       "        [0.06892909]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"computes gradient for Z= W.A_prev + b\n",
    "    Args:\n",
    "        dZ (_type_): previous gradient of Z\n",
    "        cache (_type_): All the required variables stored in forward propagation. (Z, A, A_prev, W, b)\n",
    "    Output: \n",
    "        Gradient of the cost with respect to A, W, b\n",
    "    \"\"\"\n",
    "    Z, A, A_prev, W, b = cache\n",
    "    \n",
    "    m= A_prev.shape[1]\n",
    "    \n",
    "    dW= (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db= (1/m) * np.sum(dZ, axis=1, keepdims= True)\n",
    "    dA_prev= np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"Implement the backward propagation for a single SIGMOID unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA : post-activation gradient\n",
    "    cache : All the required variables stored in forward propagation. (Z, A, A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z, A, A_prev, W, b= cache\n",
    "    \n",
    "    dZ= dA * A * (1 - A)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    \"\"\"Implement the backward propagation for a single tanh unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA : post-activation gradient\n",
    "    cache : All the required variables stored in forward propagation. (Z, A, A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z, A, A_prev, W, b= cache\n",
    "    \n",
    "    dZ= dA * A * (1-A) * (1-A)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA : post-activation gradient\n",
    "    cache:All the required variables stored in forward propagation. (Z, A, A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, A, A_prev, W, b= cache\n",
    "    dZ = np.array(dA) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z<0]=0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def leakyrelu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single  Leaky RELU unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA : post-activation gradient\n",
    "    cache:All the required variables stored in forward propagation. (Z, A, A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dZ : Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, A, A_prev, W, b= cache\n",
    "    dZ = np.array(dA) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z<0]= 0.01 \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backword(dA, cache, activation_function= 'sigmoid'):\n",
    "    # linear_cache,activation_cache=cache\n",
    "    \n",
    "    if activation_function =='relu':\n",
    "        dZ=relu_backward(dA,cache)\n",
    "        dA_prev, dW, db=linear_backward(dZ,cache)\n",
    "    \n",
    "    elif activation_function =='leakyrelu':\n",
    "        dZ=leakyrelu_backward(dA,cache)\n",
    "        dA_prev, dW, db=linear_backward(dZ,cache)\n",
    "    \n",
    "    elif activation_function =='sigmoid':\n",
    "        dZ=sigmoid_backward(dA, cache)\n",
    "        dA_prev, dW, db=linear_backward(dZ, cache)\n",
    "    \n",
    "    elif activation_function =='tanh':\n",
    "        dZ=tanh_backward(dA, cache)\n",
    "        dA_prev, dW, db=linear_backward(dZ, cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(AL, Y, caches, parameters):\n",
    "    \"\"\"computes the backpropagation of the network\n",
    "\n",
    "    Args:\n",
    "        AL : Prediction\n",
    "        Y : labels\n",
    "        caches : collection of all layer cache. layer cache ==(Z, A, A_prev, W, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    grads= {} # dictionary for all grad of the newtork\n",
    "    \n",
    "    L= len(parameters)//2 \n",
    "    \n",
    "    Y= Y.reshape(AL.shape)\n",
    "    \n",
    "    AL= -(np.divide(Y, AL) - np.divide(1-Y, 1-AL+1e-11))\n",
    "    \n",
    "    dAL= -1 * (Y/(AL+1e-11)- (1-Y)/(1-AL+1e-11))\n",
    "    \n",
    "    current_cache= caches[L-1]\n",
    "    grads['dA'+str(L-1)], grads['dW'+str(L)], grads['db'+str(L)]= linear_activation_backword(dAL, current_cache, activation_function=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads['dA'+str(l)], grads['dW'+str(l+1)], grads['db'+str(l+1)]= linear_activation_backword(grads['dA'+str(l+1)], current_cache, activation_function=\"relu\")\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA1': array([[ 3.82879788e-02],\n",
       "        [-2.98047834e-05],\n",
       "        [ 7.97851174e-03],\n",
       "        [ 3.89424802e-03],\n",
       "        [ 4.85906875e-02]]),\n",
       " 'dW2': array([[-0.00041535, -0.00128617, -0.00163443, -0.00100244, -0.00142273],\n",
       "        [ 0.0903512 ,  0.27978131,  0.35554041,  0.21806301,  0.30948829]]),\n",
       " 'db2': array([[-0.00056727],\n",
       "        [ 0.12339893]]),\n",
       " 'dA0': array([[0.01950456],\n",
       "        [0.01957727],\n",
       "        [0.02337797],\n",
       "        [0.00412003],\n",
       "        [0.02643705]]),\n",
       " 'dW1': array([[ 3.82879788e-02,  3.86708585e-02,  0.00000000e+00,\n",
       "          7.65759575e-02,  1.91439894e-01],\n",
       "        [-2.98047834e-05, -3.01028312e-05,  0.00000000e+00,\n",
       "         -5.96095667e-05, -1.49023917e-04],\n",
       "        [ 7.97851174e-03,  8.05829686e-03,  0.00000000e+00,\n",
       "          1.59570235e-02,  3.98925587e-02],\n",
       "        [ 3.89424802e-03,  3.93319050e-03,  0.00000000e+00,\n",
       "          7.78849604e-03,  1.94712401e-02],\n",
       "        [ 4.85906875e-02,  4.90765943e-02,  0.00000000e+00,\n",
       "          9.71813749e-02,  2.42953437e-01]]),\n",
       " 'db1': array([[ 3.82879788e-02],\n",
       "        [-2.98047834e-05],\n",
       "        [ 7.97851174e-03],\n",
       "        [ 3.89424802e-03],\n",
       "        [ 4.85906875e-02]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads= back_propagation(AL, Y, caches, parameters)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(X, parameters, grads, learning_rate= 0.001, lamb= 1.25):\n",
    "    \"\"\" This function will update parameter values according the gradient of the network\n",
    "    Args:\n",
    "        X : input array\n",
    "        parameters : all weight and bias parameters\n",
    "        grads (_type_): gradients of corresponding parameters\n",
    "        learning_rate (float, optional): learning rate. Defaults to 0.001.\n",
    "        lamb (float, optional): lambda for regularization. Defaults to 1.25.\n",
    "    \"\"\"\n",
    "    \n",
    "    L= len(parameters)//2\n",
    "    m= X.shape[1]\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate * (grads['dW'+str(l+1)] )#+(lamb/m)*parameters['W'+str(l+1)])\n",
    "        parameters['b'+str(l+1)]=parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.30686509, 0.11169801, 0.44643018, 0.08844759, 0.02688498],\n",
       "        [0.1638447 , 0.2558196 , 0.02770852, 0.18784861, 0.29387448],\n",
       "        [0.25076271, 0.35367595, 0.4163648 , 0.02160926, 0.44595724],\n",
       "        [0.18580762, 0.14322917, 0.00589277, 0.06285138, 0.26216919],\n",
       "        [0.10355895, 0.24541093, 0.06052531, 0.00646571, 0.42843357]]),\n",
       " 'b1': array([[0.3818147 ],\n",
       "        [0.22379793],\n",
       "        [0.21489466],\n",
       "        [0.26437758],\n",
       "        [0.78749305]]),\n",
       " 'W2': array([[0.50356556, 0.320309  , 0.52615681, 0.47563711, 0.10438969],\n",
       "        [0.3125026 , 0.00095115, 0.06671946, 0.03352666, 0.3939395 ]]),\n",
       " 'b2': array([[0.38944198],\n",
       "        [0.06880569]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_parameters(K, parameters, grads, learning_rate= 0.001, lamb= 1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Training (Forward pass + backward pass + parameter updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, num_iterations, learning_rate= 0.001, lamb= 0):\n",
    "    \"\"\"Runs all the function of the model.\n",
    "        initialize parameter ->forward propagation -> cost -> back propagation -> update parameter -> repeat.\n",
    "        \n",
    "\n",
    "    Args:\n",
    "        X :\n",
    "        Y : \n",
    "        num_iterations : iteration number\n",
    "        learning_rate (float, optional): . Defaults to 0.001.\n",
    "        lamb (int, optional): . Defaults to 0.\n",
    "    \"\"\"\n",
    "    costs= []\n",
    "    n= X.shape[0]\n",
    "    layer_dims= [n, 5, 5, 2]\n",
    "    parameters= weight_initializer(layer_dims)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # forward propagation\n",
    "        AL, caches= forward_propagation(X, parameters)\n",
    "        cost= compute_cost(AL, Y, parameters)\n",
    "        \n",
    "        #back propagation\n",
    "        grads= back_propagation(AL, Y, caches, parameters)\n",
    "        parameters= update_parameters(X, parameters, grads, learning_rate, lamb= 2)\n",
    "        \n",
    "        #printing cost\n",
    "        # if i%1000 == 0:\n",
    "        #     costs.append(cost)\n",
    "        #     print(cost)\n",
    "        costs.append(cost)\n",
    "        print(cost)\n",
    "        \n",
    "    \n",
    "    return parameters, costs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4389071667775655\n",
      "5.419422681163059\n",
      "5.400025516572007\n",
      "5.380728249850053\n",
      "5.361543576956823\n",
      "5.342484272957304\n",
      "5.323563150410922\n",
      "5.304793016494477\n",
      "5.286186629225046\n",
      "5.267756653173809\n"
     ]
    }
   ],
   "source": [
    "p, costs= optimize(K, Y, learning_rate= 0.01, num_iterations= 10, lamb=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    #m=X.shape[1]\n",
    "    #Y_prediction=np.zeros((1,m))\n",
    "    AL,_=forward_propagation (X,parameters)\n",
    "    \n",
    "    Y_prediction=np.round(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4478b0d220d70cb575cb83cb2f545b7517e2e82f9f1655a60c17255285cb4283"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
